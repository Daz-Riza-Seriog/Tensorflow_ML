{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daz-Riza-Seriog/Tensorflow_ML/blob/main/2-Customise%20your%20Models/2-%20Week%202/5-Dataset%20Class/The_Dataset_Class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WtHzzdAEeVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d038257e-81b7-4dce-d236-6c714c91a5bd"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1YMviZcEeVq"
      },
      "source": [
        "# Data Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35OO6xwgEeVq"
      },
      "source": [
        " ## Coding tutorials\n",
        " #### [1. Keras datasets](#coding_tutorial_1)\n",
        " #### [2. Dataset generators](#coding_tutorial_2)\n",
        " #### [3. Keras image data augmentation](#coding_tutorial_3)\n",
        " #### [4. The Dataset class](#coding_tutorial_4)\n",
        " #### [5. Training with Datasets](#coding_tutorial_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq5yEAJREeV9"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_4\"></a>\n",
        "## The Dataset Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVy3yC6aGdfi"
      },
      "source": [
        "#### Import the data\n",
        "\n",
        "The dataset required for this tutorial can be downloaded from the following link:\n",
        "\n",
        "https://drive.google.com/open?id=1BAjGPFlpqsDdWof50Ng3Fmju5O8F1_uZ\n",
        "\n",
        "You should store these files in Drive for use in this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJoD3fDewDfI"
      },
      "outputs": [],
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4wu2dM0wgFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7758e2b8-1386-4752-a6f8-02507118db2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title: tempest.3.2.txt, id: 1v30JPWBvWfOiQvzAwfKG9p8-mrQP5g3l\n",
            "downloading to tempest.3.2.txt\n",
            "title: tempest.4.1.txt, id: 19J-8dYnKHqUsIEzPnTL-6ZiP8e-j8-Ha\n",
            "downloading to tempest.4.1.txt\n",
            "title: tempest.5.1.txt, id: 1mNYhA47xl5zHQBDXd7fqDlvwtB-OEFvp\n",
            "downloading to tempest.5.1.txt\n",
            "title: tempest.3.3.txt, id: 1-GJTQ5DD3AFQ9NDkgOAHd3JIK9akTKLC\n",
            "downloading to tempest.3.3.txt\n",
            "title: tempest.3.1.txt, id: 1XeIYfj6VDmKamMduanCIAD0tydmnWjlP\n",
            "downloading to tempest.3.1.txt\n",
            "title: tempest.2.2.txt, id: 17Jm_sZu_ps81WiGKCCWmrfU5XADwDw--\n",
            "downloading to tempest.2.2.txt\n",
            "title: tempest.2.1.txt, id: 1aMOc8j4l1qKMEOtXv7pQDl16kYhCeqYc\n",
            "downloading to tempest.2.1.txt\n",
            "title: tempest.1.2.txt, id: 14RDC57HJud936KU0yS5JTjtbsVC5Bw2m\n",
            "downloading to tempest.1.2.txt\n",
            "title: tempest.1.1.txt, id: 1mJh2gG4wvu0Xpc9izupXio4W1eAvEFBY\n",
            "downloading to tempest.1.1.txt\n"
          ]
        }
      ],
      "source": [
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = '/content/data/shakespeare'\n",
        "try:\n",
        "    os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1BAjGPFlpqsDdWof50Ng3Fmju5O8F1_uZ' in parents\"}).GetList()  #use your own folder ID here\n",
        "\n",
        "for f in file_list:\n",
        "    # 3. Create & download by id.\n",
        "    print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "    fname = f['title']\n",
        "    print('downloading to {}'.format(fname))\n",
        "    f_ = drive.CreateFile({'id': f['id']})\n",
        "    f_.GetContentFile(\"{}/{}\".format(local_download_path,fname))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIcWGhBYGfwk"
      },
      "source": [
        "# Run this cell to connect to your Drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWNIHVSUEeV9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y92kr7WnEeV9"
      },
      "source": [
        "#### Create a simple dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHLwL0a3EeV9"
      },
      "source": [
        "x = np.zeros((100,10,2,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z72_ShgREeV9"
      },
      "source": [
        "# Create a dataset from the tensor x\n",
        "\n",
        "dataset1 = tf.data.Dataset.from_tensor_slices(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSPcILGZEeV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef214139-c3b9-4b64-9b52-7fccd7f6bcea"
      },
      "source": [
        "# Inspect the Dataset object\n",
        "\n",
        "print(dataset1)\n",
        "print(dataset1.element_spec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_TensorSliceDataset element_spec=TensorSpec(shape=(10, 2, 2), dtype=tf.float64, name=None)>\n",
            "TensorSpec(shape=(10, 2, 2), dtype=tf.float64, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WFfOYCeEeV-"
      },
      "source": [
        "x2 = [np.zeros((10,2,2)), np.zeros((5,2,2))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTCBCrDeEeV-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "3b2fd97d-f954-4bc3-bd76-4fa642a88907"
      },
      "source": [
        "# Try creating a dataset from the tensor x2\n",
        "\n",
        "dataset2 = tf.data.Dataset.from_tensor_slices(x2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e4370cf142de>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Try creating a dataset from the tensor x2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_TensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    131\u001b[0m           \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 133\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001b[0m\u001b[1;32m    134\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    342\u001b[0m                                          as_ref=False):\n\u001b[1;32m    343\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m   \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    269\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    278\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDtVyGwfEeV-"
      },
      "source": [
        "x2 = [np.zeros((10,1)), np.zeros((10,1)), np.zeros((10,1))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXSiD7_TEeV-"
      },
      "source": [
        "# Create another dataset from the new x2 and inspect the Dataset object\n",
        "\n",
        "dataset2 = tf.data.Dataset.from_tensor_slices(x2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do4wCZXzEeV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c04b877-b6cd-4f47-c154-77ba6c4b41ab"
      },
      "source": [
        "# Print the element_spec\n",
        "\n",
        "print(dataset2.element_spec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorSpec(shape=(10, 1), dtype=tf.float64, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu25rzrREeV-"
      },
      "source": [
        "#### Create a zipped dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn11lRhrEeV-"
      },
      "source": [
        "# Combine the two datasets into one larger dataset\n",
        "\n",
        "dataset_zipped = tf.data.Dataset.zip((dataset1,dataset2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHfm_yvzEeV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e4a50c-a99e-4c40-a1fb-13dd849dbf4b"
      },
      "source": [
        "# Print the element_spec\n",
        "\n",
        "print(dataset_zipped.element_spec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(10, 2, 2), dtype=tf.float64, name=None), TensorSpec(shape=(10, 1), dtype=tf.float64, name=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viTfVlfBEeV_"
      },
      "source": [
        "# Define a function to find the number of batches in a dataset\n",
        "\n",
        "def get_batches(dataset):\n",
        "    iter_dataset = iter(dataset)\n",
        "    i = 0\n",
        "    try:\n",
        "        while next(iter_dataset):\n",
        "            i = i+1\n",
        "    except:\n",
        "        return i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFkarPs9EeV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13c6193-88d8-47ce-d332-4edaed36f224"
      },
      "source": [
        "# Find the number of batches in the zipped Dataset\n",
        "\n",
        "get_batches(dataset_zipped)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkvBV4sQEeV_"
      },
      "source": [
        "#### Create a dataset from numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qo4CEQNEeV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c4ba1d-afb1-465b-9e03-3cb947983823"
      },
      "source": [
        "# Load the MNIST dataset\n",
        "\n",
        "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "print(type(train_features), type(train_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOjCtWwtEeV_"
      },
      "source": [
        "# Create a Dataset from the MNIST data\n",
        "\n",
        "mnist_dataset = tf.data.Dataset.from_tensor_slices((train_features,train_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsB-nV7KEeV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1ab7cb-a5c3-43dd-d9a3-a04eb91be97d"
      },
      "source": [
        "# Inspect the Dataset object\n",
        "\n",
        "print(mnist_dataset.element_spec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awsMdgeZEeV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53bd148-c0e6-4d2c-b210-5571748df000"
      },
      "source": [
        "# Inspect the length of an element using the take method\n",
        "\n",
        "element = next(iter(mnist_dataset.take(1)))\n",
        "print(len(element))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51unsB7LEeV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81114946-6230-46a4-9769-d3bb69cb899d"
      },
      "source": [
        "# Examine the shapes of the data\n",
        "\n",
        "print(element[0].shape)\n",
        "print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n",
            "()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFRRtfWJEeWA"
      },
      "source": [
        "#### Create a dataset from text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6ZpK2qpEeWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51eaded4-30e8-4adb-f3bf-58f6fef8bf24"
      },
      "source": [
        "# Print the list of text files\n",
        "\n",
        "text_files = sorted([f.path for f in os.scandir('data/shakespeare')])\n",
        "\n",
        "print(text_files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data/shakespeare/tempest.1.1.txt', 'data/shakespeare/tempest.1.2.txt', 'data/shakespeare/tempest.2.1.txt', 'data/shakespeare/tempest.2.2.txt', 'data/shakespeare/tempest.3.1.txt', 'data/shakespeare/tempest.3.2.txt', 'data/shakespeare/tempest.3.3.txt', 'data/shakespeare/tempest.4.1.txt', 'data/shakespeare/tempest.5.1.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCcTp2bHEeWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ecd16c3-7fa9-4a77-9607-1b5947561d0b"
      },
      "source": [
        "# Load the first file using python and print the first 5 lines.\n",
        "\n",
        "with open(text_files[0], 'r') as fil:\n",
        "    contents = [fil.readline() for i in range(5)]\n",
        "    for line in contents:\n",
        "        print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCENE I. On a ship at sea: a tempestuous noise\n",
            "\n",
            "of thunder and lightning heard.\n",
            "\n",
            "Enter a Master and a Boatswain\n",
            "\n",
            "\n",
            "\n",
            "Master\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RR4XxJ2EeWA"
      },
      "source": [
        "# Load the lines from the files into a dataset using TextLineDataset\n",
        "\n",
        "shakespeare_dataset =tf.data.TextLineDataset(text_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLqjJBeBEeWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aae2027-7b38-45b1-de74-da96df4e8380"
      },
      "source": [
        "# Use the take method to get and print the first 5 lines of the dataset\n",
        "\n",
        "first_5_lines_dataset = iter(shakespeare_dataset.take(5))\n",
        "lines = [line for line in first_5_lines_dataset]\n",
        "for line in lines:\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'SCENE I. On a ship at sea: a tempestuous noise', shape=(), dtype=string)\n",
            "tf.Tensor(b'of thunder and lightning heard.', shape=(), dtype=string)\n",
            "tf.Tensor(b'Enter a Master and a Boatswain', shape=(), dtype=string)\n",
            "tf.Tensor(b'', shape=(), dtype=string)\n",
            "tf.Tensor(b'Master', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIhYAGsEEeWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093e160c-8922-475e-d17f-484575578082"
      },
      "source": [
        "# Compute the number of lines in the first file\n",
        "\n",
        "lines = []\n",
        "with open(text_files[0], 'r') as fil:\n",
        "    line = fil.readline()\n",
        "    while line:\n",
        "        lines.append(line)\n",
        "        line = fil.readline()\n",
        "    print(len(lines))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvBb0u4UEeWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dab89d7-3143-48bc-f659-f2dfd5dc0f17"
      },
      "source": [
        "# Compute the number of lines in the shakespeare dataset we created\n",
        "\n",
        "shakespeare_dataset_iterator = iter(shakespeare_dataset)\n",
        "lines = [line for line in shakespeare_dataset_iterator]\n",
        "print(len(lines))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7ck5fkOEeWA"
      },
      "source": [
        "#### Interleave lines from the text data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cTtQV5kdEeWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50951ec-20f5-404b-f888-4295828f7011"
      },
      "source": [
        "# Create a dataset of the text file strings\n",
        "\n",
        "text_files_dataset = tf.data.Dataset.from_tensor_slices(text_files)\n",
        "files = [file for file in text_files_dataset]\n",
        "for file in files:\n",
        "    print(file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'data/shakespeare/tempest.1.1.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'data/shakespeare/tempest.1.2.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'data/shakespeare/tempest.2.1.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'data/shakespeare/tempest.2.2.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'data/shakespeare/tempest.3.1.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'data/shakespeare/tempest.3.2.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'data/shakespeare/tempest.3.3.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'data/shakespeare/tempest.4.1.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'data/shakespeare/tempest.5.1.txt', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWLOpEcWEeWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9c8a8e-eee7-4523-b7b1-860f58dce738"
      },
      "source": [
        "# Interleave the lines from the text files\n",
        "\n",
        "interleaved_shakespeare_dataset = text_files_dataset.interleave(tf.data.TextLineDataset, cycle_length=9)\n",
        "print(interleaved_shakespeare_dataset.element_spec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorSpec(shape=(), dtype=tf.string, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm5ycl5sEeWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b0bf78-dec2-437f-bb03-28fcb509e0cc"
      },
      "source": [
        "# Print the first 10 elements of the interleaved dataset\n",
        "\n",
        "lines = [line for line in iter(interleaved_shakespeare_dataset.take(10))]\n",
        "for line in lines:\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'SCENE I. On a ship at sea: a tempestuous noise', shape=(), dtype=string)\n",
            "tf.Tensor(b\"SCENE II. The island. Before PROSPERO'S cell.\", shape=(), dtype=string)\n",
            "tf.Tensor(b'SCENE I. Another part of the island.', shape=(), dtype=string)\n",
            "tf.Tensor(b'SCENE II. Another part of the island.', shape=(), dtype=string)\n",
            "tf.Tensor(b\"SCENE I. Before PROSPERO'S Cell.\", shape=(), dtype=string)\n",
            "tf.Tensor(b'SCENE II. Another part of the island.', shape=(), dtype=string)\n",
            "tf.Tensor(b'SCENE III. Another part of the island.', shape=(), dtype=string)\n",
            "tf.Tensor(b\"SCENE I. Before PROSPERO'S cell.\", shape=(), dtype=string)\n",
            "tf.Tensor(b\"SCENE I. Before PROSPERO'S cell.\", shape=(), dtype=string)\n",
            "tf.Tensor(b'of thunder and lightning heard.', shape=(), dtype=string)\n"
          ]
        }
      ]
    }
  ]
}